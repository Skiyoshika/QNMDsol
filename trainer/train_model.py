import os
import glob
import json
import numpy as np
import pandas as pd
import mne
from mne.decoding import CSP
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score, StratifiedKFold

# =================é…ç½®åŒº=================
# CSV æ–‡ä»¶æ‰€åœ¨çš„è·¯å¾„ (ç›¸å¯¹äºå½“å‰è„šæœ¬)
DATA_DIR = "../"
# é‡‡æ ·ç‡ (OpenBCI Cyton+Daisy)
SFREQ = 125 
# è¿™é‡Œçš„æ ‡ç­¾å¿…é¡»å’Œä½ å½•åˆ¶æ—¶è¾“å…¥çš„ Label ä¸€è‡´
# 0: åŸºå‡†/æ”¾æ¾, 1: åŠ¨ä½œ/æ”»å‡»
LABELS_MAP = {
    "Relax": 0,   
    "Attack": 1,  
    # "Walk": 2   # å¦‚æœä½ å½•äº† Walkï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ 
}
# ===========================================

def load_csv_data():
    """
    è‡ªåŠ¨æ‰«æå¹¶åŠ è½½ ../training_data_*.csv
    è¿”å›: X (æ•°æ®çŸ©é˜µ), y (æ ‡ç­¾åˆ—è¡¨)
    """
    print(f"ğŸ” Scanning for data in {os.path.abspath(DATA_DIR)} ...")
    
    # åŒ¹é…æ–‡ä»¶åæ¨¡å¼
    csv_files = glob.glob(os.path.join(DATA_DIR, "training_data_*.csv"))
    
    if not csv_files:
        print("âŒ Error: No CSV files found!")
        print("   -> Please run 'cargo run', switch to REAL mode, and record some data first.")
        return None, None

    all_epochs = []
    all_labels = []

    for file in csv_files:
        filename = os.path.basename(file)
        
        # 1. è‡ªåŠ¨è¯†åˆ«æ ‡ç­¾
        label = None
        for name, val in LABELS_MAP.items():
            if name.lower() in filename.lower():
                label = val
                break
        
        if label is None:
            print(f"âš ï¸ Skipping unknown file: {filename} (Label not in LABELS_MAP)")
            continue

        print(f"   -> Loading: {filename} [Label: {label}]")

        # 2. è¯»å– CSV
        try:
            df = pd.read_csv(file)
            # æˆ‘ä»¬çš„ Rust recorder è¾“å‡ºæ ¼å¼ï¼šTimestamp, Ch0, Ch1... Ch15
            # å–ç¬¬ 1 åˆ—åˆ°ç¬¬ 17 åˆ— (å…±16é€šé“)
            data = df.iloc[:, 1:17].values.T # è½¬ç½®ä¸º (n_channels, n_samples)
            
            # å•ä½è½¬æ¢: å‡è®¾ OpenBCI è¾“å‡ºæ˜¯ uV (å¾®ä¼), MNE éœ€è¦ V (ä¼ç‰¹)
            data = data * 1e-6

            # 3. åˆ‡ç‰‡ (Slicing/Epoching)
            # æŠŠé•¿é•¿çš„ä¸€æ®µå½•éŸ³åˆ‡æˆæ— æ•°ä¸ª 1ç§’ çš„å°ç‰‡æ®µç”¨äºè®­ç»ƒ
            n_channels, n_samples = data.shape
            window_size = int(SFREQ * 1.0) # 1ç§’çª—å£
            stride = int(SFREQ * 0.5)      # 0.5ç§’æ­¥é•¿ (50% é‡å )

            # å¦‚æœæ•°æ®å¤ªçŸ­ï¼Œä¸å¤Ÿåˆ‡ä¸€ç‰‡ï¼Œå°±è·³è¿‡
            if n_samples < window_size:
                continue

            for start in range(0, n_samples - window_size + 1, stride):
                end = start + window_size
                segment = data[:, start:end]
                all_epochs.append(segment)
                all_labels.append(label)
                
        except Exception as e:
            print(f"âŒ Error reading {filename}: {e}")

    if not all_epochs:
        print("âŒ Loaded files but found no valid epochs. Record longer sessions!")
        return None, None

    # è½¬æ¢ä¸º numpy æ•°ç»„: (n_epochs, n_channels, n_times)
    X = np.array(all_epochs)
    y = np.array(all_labels)
    
    return X, y

def train_and_export():
    # 1. å‡†å¤‡æ•°æ®
    X, y = load_csv_data()
    if X is None: return

    print(f"\nğŸ“Š Data Summary:")
    print(f"   Total Samples: {X.shape[0]}")
    print(f"   Channels: {X.shape[1]}")
    print(f"   Time Points: {X.shape[2]}")
    print(f"   Class Distribution: {np.bincount(y)}")

    # 2. å®šä¹‰ AI æ¨¡å‹æ¶æ„ (CSP + LDA)
    # CSP: æå–è„‘æ³¢çš„ç©ºé—´ç‰¹å¾ (è¿™æ˜¯å¤„ç†è¿åŠ¨æƒ³è±¡çš„ç¥å™¨)
    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)
    lda = LinearDiscriminantAnalysis()
    
    pipeline = Pipeline([('CSP', csp), ('LDA', lda)])

    # 3. è¯„ä¼°æ¨¡å‹ (Cross-Validation)
    # çœ‹çœ‹å¦‚æœä¸ä½œå¼Šï¼Œæ¨¡å‹èƒ½æ‰“å¤šå°‘åˆ†
    cv = StratifiedKFold(n_splits=5, shuffle=True)
    try:
        scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')
        print(f"\nğŸ† Model Accuracy: {np.mean(scores)*100:.2f}% (+/- {np.std(scores)*100:.2f}%)")
    except ValueError:
        print("\nâš ï¸ Not enough data for Cross-Validation. Training directly...")

    # 4. å…¨é‡è®­ç»ƒ
    print("ğŸš€ Training final model on full dataset...")
    pipeline.fit(X, y)

    # 5. å¯¼å‡ºæ¨¡å‹å‚æ•°åˆ° JSON
    # Rust ä¸éœ€è¦åŠ è½½æ•´ä¸ª Python å¯¹è±¡ï¼Œåªéœ€è¦çŸ©é˜µå‚æ•°åšæ•°å­¦è¿ç®—å³å¯
    
    # æå– CSP æ»¤æ³¢å™¨çŸ©é˜µ (Spatial Filters)
    filters = pipeline.named_steps['CSP'].filters_[:4] # å–å‰4ä¸ªåˆ†é‡
    
    # æå– LDA æƒé‡å’Œæˆªè·
    coef = pipeline.named_steps['LDA'].coef_[0]
    intercept = pipeline.named_steps['LDA'].intercept_[0]

    model_data = {
        "version": "1.0",
        "n_channels": 16,
        "csp_filters": filters.tolist(),
        "lda_coef": coef.tolist(),
        "lda_intercept": intercept,
        "classes": list(LABELS_MAP.keys())
    }

    output_file = "../brain_model.json"
    with open(output_file, "w") as f:
        json.dump(model_data, f, indent=4)

    print(f"\nâœ… Success! Model saved to: {os.path.abspath(output_file)}")
    print("ğŸ‘‰ Now your Rust engine can load this JSON to predict intents!")

if __name__ == "__main__":
    train_and_export()